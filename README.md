# DS-2-solution

**link**
Dataset link - https://drive.google.com/file/d/1CjoJkFuxCRqzQm10kN8UP7-osA0_x3N2/view?usp=sharing
Collab link - https://colab.research.google.com/drive/1Ummeq5iHf8bBAvmvVOnVzppeEObL8QR4?usp=sharing

Explanation_Video link - https://drive.google.com/file/d/1fYDcJOHbk5m0SDFmDdeqNcHiyKCd7nKC/view?usp=sharing




# Classification Model Comparison Project

This project aims to train, evaluate, and compare the performance of multiple classification algorithms using key evaluation metrics. It also includes hyperparameter tuning to enhance model performance and a dashboard to visualize results.

## üöÄ Objectives

- Split dataset into training and testing sets
- Train at least five classification models
- Evaluate models using:
  - Accuracy
  - Precision, Recall, F1-score
  - (Optional) R¬≤ Score
- Apply hyperparameter tuning (e.g., Grid Search, Random Search)
- Compare model performance **before and after tuning**
- Build a dashboard for visual comparison

## üß† Models Used

- Logistic Regression
- Decision Tree
- Random Forest
- Support Vector Machine (SVM)
- XGBoost

## üîß Tools & Libraries

- Python
- Pandas, NumPy
- Scikit-learn
- XGBoost
- Matplotlib / Seaborn / Plotly
- Google Colab

## üèÅ Results

- **Best Model Before Tuning:** Logistic Regression  
- **Best Model After Tuning:** XGBoost

## üìä Dashboard

The final dashboard visualizes the performance of all models, with and without tuning, using bar plots and comparison charts.

## üìå How to Run

1. Open the notebook in [Google Colab](https://colab.research.google.com/)
2. Upload the dataset (link - https://drive.google.com/file/d/1CjoJkFuxCRqzQm10kN8UP7-osA0_x3N2/view?usp=sharing)
3. Run all cells step by step
4. Review final visual comparison in the dashboard section

---

Feel free to fork, star, or use this as a base for your own classification comparison tasks!


